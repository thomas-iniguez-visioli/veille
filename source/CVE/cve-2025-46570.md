---
title: CVE-2025-46570
date: 2025-5-29
lien: "https://cvefeed.io/vuln/detail/CVE-2025-46570"

---

CVE ID : CVE-2025-46570

Published :  May 29
2025
5:15 p.m. | 5 hours
9 minutes ago

Description : vLLM is an inference and serving engine for large language models (LLMs). Prior to version 0.9.0
when a new prompt is processed
if the PageAttention mechanism finds a matching prefix chunk
the prefill process speeds up
which is reflected in the TTFT (Time to First Token). These timing differences caused by matching chunks are significant enough to be recognized and exploited. This issue has been patched in version 0.9.0.

Severity: 2.6 | LOW

Visit the link for more details
such as CVSS details
affected products
timeline
and more...
